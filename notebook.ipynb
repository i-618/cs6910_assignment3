{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics import Accuracy\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Script:\n",
    "    def __init__(self, script_name):\n",
    "        self.script_name = script_name\n",
    "        self.char2idx = {}\n",
    "        self.inx2char = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def create_vocab(self, char_list):\n",
    "        for i, char in enumerate(char_list):\n",
    "            self.char2idx[char] = i\n",
    "            self.inx2char[i] = char\n",
    "        self.vocab_size = len(char_list)\n",
    "    \n",
    "    def add_char(self, char):\n",
    "        if char not in self.char2idx:\n",
    "            self.char2idx[char] = self.vocab_size\n",
    "            self.inx2char[self.vocab_size] = char\n",
    "            self.vocab_size += 1\n",
    "        else:\n",
    "            print(\"Character already exists in the script\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asm', 'ben', 'brx', 'guj', 'hin', 'kan', 'kas', 'kok', 'mai', 'mal', 'mar', 'mni', 'ori', 'pan', 'san', 'sid', 'tam', 'tel', 'urd']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dataset_name = \"aksharantar_sampled\"\n",
    "languages_dataset = os.listdir(dataset_name)\n",
    "print(languages_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: {'y_test': 4096, 'y_train': 51200, 'y_val': 4096}\n"
     ]
    }
   ],
   "source": [
    "language = 'mal'\n",
    "START='<'\n",
    "END='>'\n",
    "def load_dataset_csv(path):\n",
    "    X, y = [], []\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(',')\n",
    "            X.append(f'{START}{line[0]}{END}')\n",
    "            y.append(f'{START}{line[1]}{END}')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "list_files = os.listdir(f'{dataset_name}/{language}')\n",
    "path = f'{dataset_name}/{language}'\n",
    "\n",
    "\n",
    "\n",
    "X_test, y_test = load_dataset_csv(f'{path}/{list_files[0]}')\n",
    "X_train, y_train = load_dataset_csv(f'{path}/{list_files[1]}')\n",
    "X_val, y_val = load_dataset_csv(f'{path}/{list_files[2]}')\n",
    "\n",
    "print('Dataset size:', {'y_test': len(y_test), 'y_train': len(y_train), 'y_val': len(y_val)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<', 1: '>', 2: 'ം', 3: 'ഃ', 4: 'അ', 5: 'ആ', 6: 'ഇ', 7: 'ഈ', 8: 'ഉ', 9: 'ഊ', 10: 'ഋ', 11: 'എ', 12: 'ഏ', 13: 'ഐ', 14: 'ഒ', 15: 'ഓ', 16: 'ഔ', 17: 'ക', 18: 'ഖ', 19: 'ഗ', 20: 'ഘ', 21: 'ങ', 22: 'ച', 23: 'ഛ', 24: 'ജ', 25: 'ഝ', 26: 'ഞ', 27: 'ട', 28: 'ഠ', 29: 'ഡ', 30: 'ഢ', 31: 'ണ', 32: 'ത', 33: 'ഥ', 34: 'ദ', 35: 'ധ', 36: 'ന', 37: 'പ', 38: 'ഫ', 39: 'ബ', 40: 'ഭ', 41: 'മ', 42: 'യ', 43: 'ര', 44: 'റ', 45: 'ല', 46: 'ള', 47: 'ഴ', 48: 'വ', 49: 'ശ', 50: 'ഷ', 51: 'സ', 52: 'ഹ', 53: 'ാ', 54: 'ി', 55: 'ീ', 56: 'ു', 57: 'ൂ', 58: 'ൃ', 59: 'െ', 60: 'േ', 61: 'ൈ', 62: 'ൊ', 63: 'ോ', 64: 'ൌ', 65: '്', 66: 'ൺ', 67: 'ൻ', 68: 'ർ', 69: 'ൽ', 70: 'ൾ'}\n",
      "{0: '<', 1: '>', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = max([len(x) for x in X_train] + [len(y) for y in y_train])\n",
    "\n",
    "unique_chars = set()\n",
    "[unique_chars.update(list(x)) for x in y_train]\n",
    "unique_chars = list(unique_chars)\n",
    "unique_chars.sort()\n",
    "\n",
    "local_script = Script(language)\n",
    "local_script.create_vocab(unique_chars)\n",
    "print(local_script.inx2char)\n",
    "\n",
    "\n",
    "\n",
    "unique_chars = set()\n",
    "[unique_chars.update(list(x)) for x in X_train]\n",
    "unique_chars = list(unique_chars)\n",
    "unique_chars.sort()\n",
    "\n",
    "latin_script = Script('latin')\n",
    "latin_script.create_vocab(unique_chars)\n",
    "print(latin_script.inx2char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transliter_pairs_test = list(zip(X_test, y_test))\n",
    "transliter_pairs_train = list(zip(X_train, y_train))\n",
    "transliter_pairs_val = list(zip(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(transliter_pairs, latin_script, local_script, batch_size=32):\n",
    "    n = len(transliter_pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=int)\n",
    "    output_ids = np.zeros((n, MAX_LENGTH), dtype=int)\n",
    "\n",
    "\n",
    "    for idx, (latin, local) in enumerate(transliter_pairs):\n",
    "        try:\n",
    "            inp_ids = [latin_script.char2idx[c] for c in latin]\n",
    "            out_ids = [local_script.char2idx[c] for c in local]\n",
    "            input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "            output_ids[idx, :len(out_ids)] = out_ids\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(output_ids).to(device))\n",
    "    sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "ValueError('could not broadcast input array from shape (32,) into shape (31,)')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n",
      "KeyError('ൗ')\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = get_dataloader(transliter_pairs_train, latin_script, local_script, batch_size=32)\n",
    "dataloader_test = get_dataloader(transliter_pairs_test, latin_script, local_script, batch_size=32)\n",
    "dataloader_val = get_dataloader(transliter_pairs_val, latin_script, local_script, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = torch.nn.functional.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = torch.nn.functional.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size=32\n",
    "encoder = Encoder(input_size=latin_script.vocab_size, hidden_size=hidden_size, dropout_p=0).to(device)\n",
    "decoder = DecoderRNN(hidden_size=hidden_size, output_size=local_script.vocab_size).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, accuracy_criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = torch.tensor([], dtype=torch.float32, device=device)\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "        accuracy = accuracy_criterion(decoded_ids, target_tensor)\n",
    "        total_accuracy = torch.cat((total_accuracy, accuracy))\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader), total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likhi\\AppData\\Local\\Temp\\ipykernel_25828\\3647478652.py:3: MatplotlibDeprecationWarning: Auto-close()ing of figures upon backend switching is deprecated since 3.8 and will be removed two minor releases later.  To suppress this warning, explicitly call plt.close('all') first.\n",
      "  plt.switch_backend('agg')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_val_loss_accuracy(val_dataloader, criterion, encoder, decoder):\n",
    "    total_loss = 0\n",
    "    for data in val_dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        # calcluating accuracy\n",
    "        \n",
    "\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100, val_dataloader=None):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    print_accuracy_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    accuracy_criterion = Accuracy(task='multiclass', num_classes=local_script.vocab_size, multidim_average='samplewise')\n",
    "\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss, accuracy = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, accuracy_criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        print_accuracy_total += sum(accuracy ==1)/len(accuracy)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_accuracy_avg = print_accuracy_total / print_every\n",
    "            print_accuracy_total = 0\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) Loss: %.4f Acc: %.2f %%' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg, print_accuracy_avg*100))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 5s (- 4m 49s) (1 2%) Loss: 0.6391 Acc: 2.25 %\n",
      "0m 11s (- 4m 26s) (2 4%) Loss: 0.5105 Acc: 3.76 %\n",
      "0m 16s (- 4m 18s) (3 6%) Loss: 0.4535 Acc: 5.03 %\n",
      "0m 22s (- 4m 18s) (4 8%) Loss: 0.4108 Acc: 6.45 %\n",
      "0m 29s (- 4m 29s) (5 10%) Loss: 0.3838 Acc: 8.30 %\n",
      "0m 36s (- 4m 31s) (6 12%) Loss: 0.3593 Acc: 9.94 %\n",
      "0m 43s (- 4m 27s) (7 14%) Loss: 0.3399 Acc: 10.96 %\n",
      "0m 49s (- 4m 17s) (8 16%) Loss: 0.3242 Acc: 13.18 %\n",
      "0m 54s (- 4m 9s) (9 18%) Loss: 0.3112 Acc: 14.11 %\n",
      "1m 0s (- 4m 0s) (10 20%) Loss: 0.2985 Acc: 14.77 %\n",
      "1m 5s (- 3m 51s) (11 22%) Loss: 0.2896 Acc: 15.70 %\n",
      "1m 10s (- 3m 42s) (12 24%) Loss: 0.2767 Acc: 17.90 %\n",
      "1m 15s (- 3m 35s) (13 26%) Loss: 0.2677 Acc: 18.07 %\n",
      "1m 20s (- 3m 27s) (14 28%) Loss: 0.2609 Acc: 18.73 %\n",
      "1m 26s (- 3m 20s) (15 30%) Loss: 0.2557 Acc: 19.41 %\n",
      "1m 31s (- 3m 14s) (16 32%) Loss: 0.2495 Acc: 21.22 %\n",
      "1m 36s (- 3m 7s) (17 34%) Loss: 0.2388 Acc: 22.49 %\n",
      "1m 42s (- 3m 1s) (18 36%) Loss: 0.2324 Acc: 22.61 %\n",
      "1m 47s (- 2m 55s) (19 38%) Loss: 0.2297 Acc: 23.14 %\n",
      "1m 52s (- 2m 49s) (20 40%) Loss: 0.2226 Acc: 23.63 %\n",
      "1m 58s (- 2m 43s) (21 42%) Loss: 0.2160 Acc: 24.66 %\n",
      "2m 3s (- 2m 37s) (22 44%) Loss: 0.2115 Acc: 24.98 %\n",
      "2m 9s (- 2m 31s) (23 46%) Loss: 0.2094 Acc: 25.73 %\n",
      "2m 14s (- 2m 26s) (24 48%) Loss: 0.1998 Acc: 27.32 %\n",
      "2m 20s (- 2m 20s) (25 50%) Loss: 0.1961 Acc: 28.30 %\n",
      "2m 25s (- 2m 14s) (26 52%) Loss: 0.1915 Acc: 28.74 %\n",
      "2m 31s (- 2m 8s) (27 54%) Loss: 0.1891 Acc: 29.74 %\n",
      "2m 36s (- 2m 3s) (28 56%) Loss: 0.1826 Acc: 30.98 %\n",
      "2m 42s (- 1m 57s) (29 57%) Loss: 0.1802 Acc: 31.47 %\n",
      "2m 47s (- 1m 51s) (30 60%) Loss: 0.1764 Acc: 30.91 %\n",
      "2m 53s (- 1m 46s) (31 62%) Loss: 0.1728 Acc: 32.13 %\n",
      "2m 58s (- 1m 40s) (32 64%) Loss: 0.1654 Acc: 34.55 %\n",
      "3m 4s (- 1m 35s) (33 66%) Loss: 0.1664 Acc: 33.62 %\n",
      "3m 10s (- 1m 29s) (34 68%) Loss: 0.1625 Acc: 34.40 %\n",
      "3m 15s (- 1m 23s) (35 70%) Loss: 0.1601 Acc: 33.91 %\n",
      "3m 21s (- 1m 18s) (36 72%) Loss: 0.1638 Acc: 33.03 %\n",
      "3m 27s (- 1m 12s) (37 74%) Loss: 0.1558 Acc: 34.91 %\n",
      "3m 32s (- 1m 7s) (38 76%) Loss: 0.1507 Acc: 36.38 %\n",
      "3m 38s (- 1m 1s) (39 78%) Loss: 0.1480 Acc: 37.16 %\n",
      "3m 43s (- 0m 55s) (40 80%) Loss: 0.1429 Acc: 38.43 %\n",
      "3m 49s (- 0m 50s) (41 82%) Loss: 0.1406 Acc: 38.26 %\n",
      "3m 54s (- 0m 44s) (42 84%) Loss: 0.1395 Acc: 38.09 %\n",
      "4m 0s (- 0m 39s) (43 86%) Loss: 0.1420 Acc: 36.13 %\n",
      "4m 5s (- 0m 33s) (44 88%) Loss: 0.1391 Acc: 38.06 %\n",
      "4m 10s (- 0m 27s) (45 90%) Loss: 0.1320 Acc: 40.31 %\n",
      "4m 16s (- 0m 22s) (46 92%) Loss: 0.1272 Acc: 41.53 %\n",
      "4m 21s (- 0m 16s) (47 94%) Loss: 0.1257 Acc: 42.43 %\n",
      "4m 27s (- 0m 11s) (48 96%) Loss: 0.1206 Acc: 42.53 %\n",
      "4m 33s (- 0m 5s) (49 98%) Loss: 0.1212 Acc: 43.43 %\n",
      "4m 39s (- 0m 0s) (50 100%) Loss: 0.1252 Acc: 41.14 %\n"
     ]
    }
   ],
   "source": [
    "loss = train(dataloader_val, encoder, decoder, 50, print_every=1, plot_every=1)\n",
    "\n",
    "# for i in range(1000):\n",
    "#     losss = train_epoch(dataloader, encoder, decoder, torch.optim.Adam(encoder.parameters()), torch.optim.Adam(decoder.parameters()), torch.nn.NLLLoss())\n",
    "#     print(losss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = iter(dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['<bite>', '<economicsil>', '<chuttiyulla>', '<thuvvoor>', '<kanamenna>', '<aashupathriyakk>', '<platto>', '<vayattil>', '<libraryyil>', '<parichunakkiya>', '<grandhathile>', '<ne>', '<ulppedunnathaanithu>', '<baraakk>', '<oven>', '<eraakhile>', '<hanumaanum>', '<chinayil>', '<laantilum>', '<elaktro>', '<vyathyasthathakalaanu>', '<praathinidhya>', '<variyaayi>', '<sahanasamaravum>', '<vistheernam>', '<kazhiyunnavarille>', '<paass>', '<leghayude>', '<mathsaramaayirunnu>', '<sanghadippikkaan>', '<kongressile>', '<aalathur>']\n",
      "Expected: ['<ബൈറ്റ്>', '<ഇക്കണോമിക്സിൽ>', '<ചുറ്റിയുള്ള>', '<തുവ്വൂർ>', '<കാണാമെന്ന>', '<ആശുപത്രിയക്ക്>', '<പ്ലേറ്റോ>', '<വയറ്റിൽ>', '<ലൈബ്രറിയിൽ>', '<പറിച്ചുണക്കിയ>', '<ഗ്രന്ഥത്തിലെ>', '<നേ>', '<ഉൾപ്പെടുന്നതാണിത്>', '<ബരാക്ക്>', '<ഓവൻ>', '<ഇറാഖിലെ>', '<ഹനുമാനും>', '<ചൈനയിൽ>', '<ലാന്റിലും>', '<ഇലക്ട്രോ>', '<വ്യത്യസ്ഥതകളാണ്>', '<പ്രാതിനിധ്യ>', '<വരിയായി>', '<സഹനസമരവും>', '<വിസ്തീർണം>', '<കഴിയുന്നവരില്ലേ>', '<പാസ്സ്>', '<ലേഖയുടെ>', '<മത്സരമായിരുന്നു>', '<സംഘടിപ്പിക്കാൻ>', '<കോൺഗ്രസ്സിലെ>', '<ആലത്തൂർ>']\n",
      "Predicted: ['<ബീറ്റ്>', '<ഇക്കണോൾസിലായ>', '<ചുറ്റിലും>', '<തുവ്വേറ്റ്>', '<കാണാമെന്ന>', '<ആകാരസ്സരിച്ച്>', '<പ്രസ്സ്>', '<വാട്യൂബി>', '<ലൈബ്രറിയിൽ>', '<പരിപാലിക്കുന്നത്>', '<ഗ്രന്ഥത്തിലെ>', '<നേ>', '<ഉദ്യപ്പിക്കുന്നവരാണ്>', '<ബരാക്ക്>', '<ഓവൻ>', '<ഇറാഖിലെ>', '<ഹനുമാനും>', '<ചൈനയിൽ>', '<ലൈറാകൾ>', '<ഇലക്ട്രോ>', '<വ്യത്യസ്ഥതകളാണ്>', '<പ്രാതിനിടാം>', '<വരികോട്>', '<സഹായമദാക്ഷത്തി>', '<വിദ്യോഗസ്ഥൻ>', '<കഴിയുവേണ്ടിച്ചവരെ>', '<പാസ്സ്>', '<ലൈംഗോവണം>', '<മത്സരമായിരുന്നു>', '<സംഘടിപ്പിക്കാൻ>', '<കോൺഗ്രസ്സിലെ>', '<ആദ്യാവി>']\n",
      "Accuracy:   0.46875\n",
      "Matched:  {'<മത്സരമായിരുന്നു>', '<ലൈബ്രറിയിൽ>', '<കാണാമെന്ന>', '<പാസ്സ്>', '<ഓവൻ>', '<വ്യത്യസ്ഥതകളാണ്>', '<കോൺഗ്രസ്സിലെ>', '<ഹനുമാനും>', '<ഇലക്ട്രോ>', '<ബരാക്ക്>', '<നേ>', '<ഗ്രന്ഥത്തിലെ>', '<സംഘടിപ്പിക്കാൻ>', '<ഇറാഖിലെ>', '<ചൈനയിൽ>'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_tensor_to_string(tensor, script):\n",
    "    words = []\n",
    "    for idx in tensor:\n",
    "        word = []\n",
    "        for i in idx:\n",
    "            word.append(script.inx2char[i.item()])\n",
    "            if i.item() == script.char2idx[END]:\n",
    "                break\n",
    "        words.append(''.join(word))\n",
    "    return words\n",
    "\n",
    "input_tensor, target_tensor = next(test_data)\n",
    "encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "_, topi = decoder_outputs.topk(1)\n",
    "decoded_ids = topi.squeeze()\n",
    "\n",
    "input_words, output_words = convert_tensor_to_string(input_tensor, latin_script), convert_tensor_to_string(decoded_ids, local_script)\n",
    "expected_words = convert_tensor_to_string(target_tensor, local_script)\n",
    "\n",
    "print('Input:', input_words)\n",
    "print('Expected:', expected_words)\n",
    "print('Predicted:', output_words)\n",
    "\n",
    "matched_words = set(expected_words) & set(output_words)\n",
    "print('Accuracy:  ', len(matched_words)/ len(expected_words))\n",
    "print('Matched: ', matched_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(torch.nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = torch.nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = torch.nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(0)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = torch.nn.functional.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_size=latin_script.vocab_size, hidden_size=hidden_size, dropout_p=0).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size=hidden_size, output_size=local_script.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 11s (- 1m 46s) (1 10%) Loss: 1.2378 Acc: 0.71 %\n",
      "0m 23s (- 1m 33s) (2 20%) Loss: 0.8515 Acc: 0.71 %\n",
      "0m 34s (- 1m 21s) (3 30%) Loss: 0.6889 Acc: 0.76 %\n",
      "0m 46s (- 1m 10s) (4 40%) Loss: 0.4615 Acc: 3.34 %\n",
      "0m 58s (- 0m 58s) (5 50%) Loss: 0.2854 Acc: 10.42 %\n",
      "1m 9s (- 0m 46s) (6 60%) Loss: 0.2007 Acc: 19.65 %\n",
      "1m 20s (- 0m 34s) (7 70%) Loss: 0.1592 Acc: 26.56 %\n",
      "1m 31s (- 0m 22s) (8 80%) Loss: 0.1360 Acc: 32.18 %\n",
      "1m 43s (- 0m 11s) (9 90%) Loss: 0.1163 Acc: 36.60 %\n",
      "1m 54s (- 0m 0s) (10 100%) Loss: 0.1015 Acc: 41.46 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2377742053940892,\n",
       " 0.8515051216818392,\n",
       " 0.688946230802685,\n",
       " 0.461546499049291,\n",
       " 0.2854411626467481,\n",
       " 0.2007488302187994,\n",
       " 0.15915535757085308,\n",
       " 0.1360222269431688,\n",
       " 0.116289580357261,\n",
       " 0.10148271830985323]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(dataloader_val, encoder, attn_decoder, 10, print_every=1, plot_every=1, val_dataloader=dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['<gavanmentinu>', '<chikithsichumaattaam>', '<sadanam>', '<varikalaanu>', '<kombukalum>', '<kanjangad>', '<maidrid>', '<jorj>', '<manthrisabhaayogathine>', '<swadeshiyum>', '<anusarichum>', '<aswasthathayoyaanu>', '<faashisttaaya>', '<karnaadakathilum>', '<auak>', '<bora>', '<aashankaanimisham>', '<ruupappedunnathu>', '<chinayilum>', '<panjcha>', '<grafic>', '<vimanangale>', '<samarthamaaya>', '<dikkil>', '<adukkurippodeyaanu>', '<midhyayo>', '<thudarendennu>', '<aadyathil>', '<pappanum>', '<aduthethaanaakoo>', '<raashtreeyabhethamanye>', '<indirayude>']\n",
      "Expected: ['<ഗവൺമെന്റിന്>', '<ചികിത്സിച്ചുമാറ്റാം>', '<സദനം>', '<വരികളാണ്>', '<കൊമ്പുകളും>', '<കാഞ്ഞങ്ങാട്>', '<മാഡ്രിഡ്>', '<ജോര്ജ്>', '<മന്ത്രിസഭായോഗത്തിനേ>', '<സ്വദേശിയും>', '<അനുസരിച്ചും>', '<അസ്വസ്ഥതയോയാണ്>', '<ഫാഷിസ്റ്റായ>', '<കർണാടകത്തിലും>', '<ഓക്ക്>', '<ബോറ>', '<ആശങ്കാനിമിഷം>', '<രൂപപ്പെടുന്നത്>', '<ചൈനയിലും>', '<പഞ്ച>', '<ഗ്രാഫിക്>', '<വിമാനങ്ങളെ>', '<സമർഥമായ>', '<ദിക്കിൽ>', '<അടുക്കുറിപ്പോടെയാണ്>', '<മിഥ്യയോ>', '<തുടരേണ്ടെന്നു>', '<ആദ്യത്തിൽ>', '<പപ്പനും>', '<അടുത്തെത്താനാകൂ>', '<രാഷ്ട്രീയഭേതമന്യേ>', '<ഇന്ദിരയുടെ>']\n",
      "Predicted: ['<ഗവന്മെന്റിന്>', '<ചികിത്സിച്ചുമാട്>', '<സദനം>', '<വരികളാണ്>', '<കൊമ്പുകളും>', '<കഞ്ഞങ്ങട്>', '<മൈഡ്രിഡ്>', '<ജോർജ്>', '<മന്ത്രിസഭായോഗത്തിനെ>', '<സ്വദേശിയും>', '<അനുസരിച്ചും>', '<അസ്വസ്ഥതയോയാണ്>', '<ഫാഷിസ്റ്റായ>', '<കർണാടകത്തിലും>', '<ആക്ക്>', '<ബോറ>', '<ആശങ്കാനിമിഷം>', '<രൂപപ്പെടുന്നത്>', '<ചിനയിലും>', '<പഞ്ച>', '<ഗ്രാഫിക്>', '<വിമാനങ്ങളെ>', '<സമർതമായ>', '<ഡിക്കിൽ>', '<അടുക്കുറിപ്പോടെയാണ്>', '<മിധ്യയോ>', '<തുടരെന്ദെന്നു>', '<ആദ്യത്തിൽ>', '<പപ്പനും>', '<അടുത്തേത്താനാകൂകോ>', '<രാശ്ത്രീയഭേതമനെ>', '<ഇന്ദിരയുടെ>']\n",
      "Accuracy:   0.5625\n",
      "Matched:  {'<വരികളാണ്>', '<ആദ്യത്തിൽ>', '<അസ്വസ്ഥതയോയാണ്>', '<കൊമ്പുകളും>', '<ഫാഷിസ്റ്റായ>', '<സദനം>', '<രൂപപ്പെടുന്നത്>', '<ഗ്രാഫിക്>', '<ആശങ്കാനിമിഷം>', '<അടുക്കുറിപ്പോടെയാണ്>', '<പഞ്ച>', '<അനുസരിച്ചും>', '<ബോറ>', '<പപ്പനും>', '<ഇന്ദിരയുടെ>', '<വിമാനങ്ങളെ>', '<കർണാടകത്തിലും>', '<സ്വദേശിയും>'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_tensor_to_string(tensor, script):\n",
    "    words = []\n",
    "    for idx in tensor:\n",
    "        word = []\n",
    "        for i in idx:\n",
    "            word.append(script.inx2char[i.item()])\n",
    "            if i.item() == script.char2idx[END]:\n",
    "                break\n",
    "        words.append(''.join(word))\n",
    "    return words\n",
    "\n",
    "input_tensor, target_tensor = next(test_data)\n",
    "encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "decoder_outputs, _, _ = attn_decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "_, topi = decoder_outputs.topk(1)\n",
    "decoded_ids = topi.squeeze()\n",
    "\n",
    "input_words, output_words = convert_tensor_to_string(input_tensor, latin_script), convert_tensor_to_string(decoded_ids, local_script)\n",
    "expected_words = convert_tensor_to_string(target_tensor, local_script)\n",
    "\n",
    "print('Input:', input_words)\n",
    "print('Expected:', expected_words)\n",
    "print('Predicted:', output_words)\n",
    "\n",
    "matched_words = set(expected_words) & set(output_words)\n",
    "print('Accuracy:  ', len(matched_words)/ len(expected_words))\n",
    "print('Matched: ', matched_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
